"""Tests for remote provider implementation"""

import pytest
import json
import logging
from unittest.mock import Mock, patch, AsyncMock, MagicMock
from datetime import datetime

from llama_stack_provider_trustyai_garak.remote import get_adapter_impl
from llama_stack_provider_trustyai_garak.remote.garak_remote_eval import GarakRemoteEvalAdapter
from llama_stack_provider_trustyai_garak.remote.provider import get_provider_spec
from llama_stack_provider_trustyai_garak.config import GarakRemoteConfig, KubeflowConfig, GarakScanConfig
from llama_stack_provider_trustyai_garak.errors import GarakConfigError, GarakValidationError
from llama_stack_provider_trustyai_garak.compat import Api, JobStatus, EvaluateResponse, Benchmark

class TestRemoteProvider:
    """Test cases for remote provider specification"""

    def test_get_provider_spec(self):
        """Test provider specification"""
        spec = get_provider_spec()
        
        assert spec.api == Api.eval
        assert spec.adapter_type == "trustyai_garak"
        
        # Check for garak (may have version specifier like garak==0.12.0)
        assert any(pkg.startswith("garak") for pkg in spec.pip_packages), "garak not found in pip_packages"
        
        # Check for other packages (exact match)
        for package in ["kfp", "kfp-kubernetes", "kfp-server-api", "boto3"]:
            assert package in spec.pip_packages, f"{package} not found in pip_packages"
        assert spec.config_class == "llama_stack_provider_trustyai_garak.config.GarakRemoteConfig"
        assert spec.module == "llama_stack_provider_trustyai_garak.remote"
        for api in [Api.inference, Api.files, Api.benchmarks]:
            assert api in spec.api_dependencies, f"{api} not found in api_dependencies"
        for api in [Api.safety, Api.shields]:
            assert api in spec.optional_api_dependencies, f"{api} not found in optional_api_dependencies"


class TestRemoteAdapterCreation:
    """Test cases for remote adapter creation"""

    @pytest.mark.asyncio
    async def test_get_adapter_impl_success(self):
        """Test successful adapter implementation creation"""
        kubeflow_config = KubeflowConfig(
            pipelines_endpoint="https://kfp.example.com",
            namespace="default",
            base_image="test:latest"
        )
        config = GarakRemoteConfig(kubeflow_config=kubeflow_config)
        
        mock_deps = {
            Api.files: Mock(),
            Api.benchmarks: Mock()
        }
        
        with patch.object(GarakRemoteEvalAdapter, 'initialize', new_callable=AsyncMock):
            with patch.object(GarakRemoteEvalAdapter, '_ensure_garak_installed'):
                with patch.object(GarakRemoteEvalAdapter, '_get_all_probes', return_value=set()):
                    with patch.object(GarakRemoteEvalAdapter, '_create_kfp_client'):
                        impl = await get_adapter_impl(config, mock_deps)
                        
                        assert isinstance(impl, GarakRemoteEvalAdapter)
                        assert impl._config == config
                        impl.initialize.assert_called_once()

    @pytest.mark.asyncio
    async def test_get_adapter_impl_with_optional_deps(self):
        """Test adapter implementation with optional safety and shields dependencies"""
        kubeflow_config = KubeflowConfig(
            pipelines_endpoint="https://kfp.example.com",
            namespace="default",
            base_image="test:latest"
        )
        config = GarakRemoteConfig(kubeflow_config=kubeflow_config)
        
        mock_deps = {
            Api.files: Mock(),
            Api.benchmarks: Mock(),
            Api.safety: Mock(),
            Api.shields: Mock()
        }
        
        with patch.object(GarakRemoteEvalAdapter, 'initialize', new_callable=AsyncMock):
            with patch.object(GarakRemoteEvalAdapter, '_ensure_garak_installed'):
                with patch.object(GarakRemoteEvalAdapter, '_get_all_probes', return_value=set()):
                    with patch.object(GarakRemoteEvalAdapter, '_create_kfp_client'):
                        impl = await get_adapter_impl(config, mock_deps)
                        
                        assert impl.safety_api is not None
                        assert impl.shields_api is not None

    @pytest.mark.asyncio
    async def test_get_adapter_impl_error_handling(self):
        """Test error handling in adapter implementation creation"""
        kubeflow_config = KubeflowConfig(
            pipelines_endpoint="https://kfp.example.com",
            namespace="default",
            base_image="test:latest"
        )
        config = GarakRemoteConfig(kubeflow_config=kubeflow_config)
        
        mock_deps = {
            Api.files: Mock(),
            Api.benchmarks: Mock()
        }
        
        with patch.object(GarakRemoteEvalAdapter, 'initialize', new_callable=AsyncMock) as mock_init:
            mock_init.side_effect = Exception("Initialization failed")
            
            with pytest.raises(Exception) as exc_info:
                await get_adapter_impl(config, mock_deps)
            
            assert "Initialization failed" in str(exc_info.value)


class TestGarakRemoteEvalAdapter:
    """Test cases for GarakRemoteEvalAdapter"""

    @pytest.fixture
    def adapter_config(self):
        """Create test configuration"""
        kubeflow_config = KubeflowConfig(
            pipelines_endpoint="https://kfp.example.com",
            namespace="test-namespace",
            base_image="test:latest"
        )
        return GarakRemoteConfig(
            kubeflow_config=kubeflow_config,
            llama_stack_url="http://test.api.com/v1"
        )

    @pytest.fixture
    def mock_deps(self, mock_file_api, mock_benchmarks_api):
        """Create mock dependencies"""
        return {
            Api.files: mock_file_api,
            Api.benchmarks: mock_benchmarks_api
        }

    @pytest.fixture
    def adapter(self, adapter_config, mock_deps):
        """Create adapter instance"""
        return GarakRemoteEvalAdapter(adapter_config, mock_deps)

    @pytest.fixture
    def mock_benchmark(self):
        """Create a mock Benchmark object"""
        benchmark = Mock()
        benchmark.identifier = "test-benchmark"
        benchmark.metadata = {"probes": ["probe1", "probe2"]}
        return benchmark

    @pytest.fixture
    def mock_benchmark_config(self):
        """Create a real BenchmarkConfig object"""
        # Import the actual classes
        from llama_stack_provider_trustyai_garak.compat import (
            BenchmarkConfig,
            SamplingParams,
            TopPSamplingStrategy
        )
        
        # Create a real BenchmarkConfig with required fields
        config = BenchmarkConfig(
            eval_candidate={
                "type": "model",
                "model": "test-model",
                "sampling_params": SamplingParams(
                    strategy=TopPSamplingStrategy(temperature=0.7, top_p=0.95),
                    max_tokens=100
                )
            }
        )
        return config

    @pytest.mark.asyncio
    async def test_initialize(self, adapter, adapter_config):
        """Test adapter initialization"""
        with patch.object(adapter, '_ensure_garak_installed'):
            with patch.object(adapter, '_get_all_probes', return_value={'probe1', 'probe2'}):
                with patch.object(adapter, '_create_kfp_client'):
                    await adapter.initialize()
                    
                    assert adapter._initialized is True
                    assert adapter.all_probes == {'probe1', 'probe2'}
                    assert adapter._verify_ssl == adapter_config.tls_verify

    @pytest.mark.asyncio
    async def test_initialize_with_mismatched_safety_shields(self, adapter):
        """Test initialization fails when only one of safety/shields is provided"""
        adapter.shields_api = Mock()
        adapter.safety_api = None
        
        with patch.object(adapter, '_ensure_garak_installed'):
            with patch.object(adapter, '_get_all_probes', return_value=set()):
                with pytest.raises(GarakConfigError) as exc_info:
                    await adapter.initialize()
                
                assert "Shields API is provided but Safety API is not provided" in str(exc_info.value)


    def test_create_kfp_client_success(self, adapter):
        """Test successful KFP client creation"""
        # Create mocks for kfp
        mock_kfp = MagicMock()
        mock_kfp_client = Mock()
        mock_kfp.Client.return_value = mock_kfp_client
        
        # Mock kfp_server_api
        mock_kfp_server = MagicMock()
        
        # Fix: Set _verify_ssl before testing (it's set in initialize())
        adapter._verify_ssl = True
        
        with patch.dict('sys.modules', {'kfp': mock_kfp, 'kfp_server_api': mock_kfp_server, 'kfp_server_api.exceptions': mock_kfp_server.exceptions}):
            with patch.object(adapter, '_get_token', return_value="test-token"):
                # Import and use the mocked kfp
                from kfp import Client
                adapter.kfp_client = Client(
                    host=adapter._config.kubeflow_config.pipelines_endpoint,
                    existing_token="test-token",
                    verify_ssl=adapter._verify_ssl,
                    ssl_ca_cert=None
                )
                
                assert adapter.kfp_client == mock_kfp_client

    def test_resolve_framework_to_probes(self, adapter):
        """Test resolving framework ID to probes for remote provider"""
        adapter.scan_config = GarakScanConfig()
        
        # Remote provider returns ['all'] and sets probe_tags in metadata
        # Actual probe resolution happens in the KFP pod
        probes = adapter._resolve_framework_to_probes('trustyai_garak::owasp_llm_top10')
            
        # Remote mode returns ['all'] - filtering via probe_tags
        assert probes == ['all']

    def test_resolve_framework_to_probes_unknown_framework_raises(self, adapter):
        """Unknown framework_id should raise GarakValidationError in remote mode."""
        adapter.scan_config = GarakScanConfig()
        
        with pytest.raises(GarakValidationError) as exc_info:
            adapter._resolve_framework_to_probes("nonexistent_framework_id")
        
        assert "Unknown framework" in str(exc_info.value)

    @pytest.mark.asyncio
    async def test_register_benchmark_enriches_metadata_with_probe_tags_remote(self, adapter):
        """register_benchmark should copy taxonomy_filters to metadata['probe_tags'] and persist."""
        adapter.scan_config = GarakScanConfig()
        
        # Mock the benchmarks_api to avoid actual API calls
        adapter.benchmarks_api = Mock()
        
        framework_id = "trustyai_garak::owasp_llm_top10"
        taxonomy_filters = adapter.scan_config.FRAMEWORK_PROFILES[framework_id]["taxonomy_filters"]
        
        # Create a benchmark for a framework-based profile
        benchmark = Benchmark(
            identifier=framework_id,
            dataset_id="garak",
            scoring_functions=["garak_scoring"],
            provider_id="trustyai_garak_remote",
            provider_benchmark_id=framework_id,
            metadata={}  # Empty metadata - should be enriched
        )
        
        # Register the benchmark
        await adapter.register_benchmark(benchmark)
        
        # Benchmark metadata should include probes=['all'] and probe_tags from the framework profile
        assert "probes" in benchmark.metadata
        assert benchmark.metadata["probes"] == ['all']
        assert "probe_tags" in benchmark.metadata
        assert benchmark.metadata["probe_tags"] == taxonomy_filters
        
        # Adapter should persist the enriched benchmark in its internal storage
        stored_benchmark = adapter.benchmarks[framework_id]
        assert stored_benchmark.metadata["probe_tags"] == taxonomy_filters
        assert stored_benchmark.metadata["probes"] == ['all']

    def test_get_job_id(self, adapter):
        """Test job ID generation"""
        with patch('uuid.uuid4', return_value='test-uuid'):
            job_id = adapter._get_job_id()
            
            assert job_id.startswith('garak-job-')
            assert 'test-uuid' in job_id

    @pytest.mark.asyncio
    async def test_register_benchmark(self, adapter, mock_benchmark):
        """Test benchmark registration"""
        await adapter.register_benchmark(mock_benchmark)
        
        assert adapter.benchmarks["test-benchmark"] == mock_benchmark

    @pytest.mark.asyncio
    async def test_register_predefined_benchmark(self, adapter):
        """Test registering a pre-defined benchmark"""
        adapter.scan_config = GarakScanConfig()
        
        # Create a mock benchmark for predefined profiles
        benchmark = Mock()
        benchmark.identifier = "trustyai_garak::quick"
        benchmark.metadata = None
        
        with patch.object(adapter, '_resolve_framework_to_probes', return_value=['probe1']):
            await adapter.register_benchmark(benchmark)
            
            # Check that metadata was set from the predefined profile
            assert benchmark.metadata is not None

    @pytest.mark.asyncio
    async def test_build_command_basic(self, adapter, mock_benchmark, mock_benchmark_config):
        """Test basic command building"""
        scan_profile_config = {
            "probes": ["probe1", "probe2"],
            "timeout": 3600
        }
        
        # Mock necessary methods
        adapter.all_probes = {"probe1", "probe2"}
        
        with patch.object(adapter, 'get_benchmark', return_value=mock_benchmark):
            with patch.object(adapter, '_get_generator_options', return_value={"test": "options"}):
                cmd = await adapter._build_command(
                    mock_benchmark_config,
                    "test-benchmark",
                    scan_profile_config
                )
                
                assert "garak" in cmd
                assert "--model_type" in cmd
                assert "--model_name" in cmd
                assert "--generator_options" in cmd
                assert "--probes" in cmd
                assert "probe1,probe2" in cmd

    @pytest.mark.asyncio
    async def test_build_command_with_shields(self, adapter, mock_benchmark_config):
        """Test command building with shields"""
        benchmark = Mock()
        benchmark.identifier = "test-benchmark"
        benchmark.metadata = {
            "probes": ["probe1"],
            "shield_ids": ["shield1", "shield2"]
        }
        
        scan_profile_config = {"probes": ["probe1"], "timeout": 3600}
        
        adapter.all_probes = {"probe1"}
        
        with patch.object(adapter, 'get_benchmark', return_value=benchmark):
            with patch.object(adapter, '_get_function_based_generator_options', return_value={"function": "options"}):
                cmd = await adapter._build_command(
                    mock_benchmark_config,
                    "test-benchmark",
                    scan_profile_config
                )
                
                assert "--model_type" in cmd
                assert "function.Single" in cmd

    @pytest.mark.asyncio
    async def test_run_eval_success(self, adapter, mock_benchmark, mock_benchmark_config):
        """Test successful evaluation run"""
        adapter.all_probes = {"probe1"}
        adapter._initialized = True
        # Fix: Set _verify_ssl which is normally set during initialization
        adapter._verify_ssl = True
        
        mock_run = Mock()
        mock_run.run_id = "test-run-id"
        mock_run.run_info.created_at = datetime.now()
        
        with patch.object(adapter, 'get_benchmark', return_value=mock_benchmark):
            with patch.object(adapter, '_build_command', return_value=["garak", "--probes", "probe1"]):
                # Mock the kfp_client
                adapter.kfp_client = Mock()
                adapter.kfp_client.create_run_from_pipeline_func.return_value = mock_run
                
                result = await adapter.run_eval("test-benchmark", mock_benchmark_config)
                
                assert "job_id" in result
                assert result["status"] == JobStatus.scheduled
                assert "metadata" in result
                # Verify the job_id is properly stored
                job_id = result["job_id"]
                assert job_id in adapter._jobs
                assert adapter._jobs[job_id].status == JobStatus.scheduled
                assert adapter._job_metadata[job_id]["kfp_run_id"] == "test-run-id"

    @pytest.mark.asyncio
    async def test_run_eval_invalid_candidate_type(self, adapter, mock_benchmark):
        """Test run_eval with invalid candidate type"""
        # Create a mock config that will fail the type check
        invalid_config = Mock()
        invalid_config.eval_candidate = Mock()
        invalid_config.eval_candidate.type = "invalid"  # This should be "model"
        invalid_config.eval_candidate.model = "test-model"
        
        # Make it not an instance of BenchmarkConfig so it fails isinstance check
        adapter._initialized = True
        
        with pytest.raises(GarakValidationError) as exc_info:
            await adapter.run_eval("test-benchmark", invalid_config)
        
        assert "Required benchmark_config to be of type BenchmarkConfig" in str(exc_info.value)

    @pytest.mark.asyncio
    async def test_job_status_in_progress(self, adapter):
        """Test getting status of in-progress job"""
        job_id = "test-job-id"
        adapter._jobs[job_id] = Mock(status=JobStatus.scheduled)
        adapter._job_metadata[job_id] = {"kfp_run_id": "test-run-id"}
        
        mock_run = Mock()
        mock_run.state = "RUNNING"
        
        adapter.kfp_client = Mock()
        adapter.kfp_client.get_run.return_value = mock_run
        
        # Mock the mapping function
        with patch.object(adapter, '_map_kfp_run_state_to_job_status', return_value=JobStatus.in_progress):
            result = await adapter.job_status("test-benchmark", job_id)
            
            assert result["job_id"] == job_id
            assert result["status"] == JobStatus.in_progress

    @pytest.mark.asyncio
    async def test_job_status_completed(self, adapter):
        """Test getting status of completed job"""
        mock_run = Mock()
        mock_run.state = "SUCCEEDED"
        mock_run.finished_at = datetime.now()
        
        job_id = "test-job-id"
        adapter._jobs[job_id] = Mock(status=JobStatus.in_progress)
        adapter._job_metadata[job_id] = {"kfp_run_id": "test-run-id"}
        
        adapter.kfp_client = Mock()
        adapter.kfp_client.get_run.return_value = mock_run
        
        # Mock Files API response for mapping file retrieval
        from llama_stack_provider_trustyai_garak.compat import OpenAIFilePurpose
        
        mock_file_list = Mock()
        mock_file_obj = Mock()
        mock_file_obj.filename = f"{job_id}_mapping.json"
        mock_file_obj.id = "mapping-file-id-123"
        mock_file_list.data = [mock_file_obj]
        
        mock_mapping_content = Mock()
        mock_mapping_content.body.decode.return_value = json.dumps({
            f"{job_id}_scan_result.json": "file-id-123"
        })
        
        adapter.file_api.openai_list_files = AsyncMock(return_value=mock_file_list)
        adapter.file_api.openai_retrieve_file_content = AsyncMock(return_value=mock_mapping_content)
        
        with patch.object(adapter, '_map_kfp_run_state_to_job_status', return_value=JobStatus.completed):
            result = await adapter.job_status("test-benchmark", job_id)
            
            assert result["status"] == JobStatus.completed
            assert adapter._job_metadata[job_id].get(f"{job_id}_scan_result.json") == "file-id-123"
            # Verify mapping_file_id was cached
            assert adapter._job_metadata[job_id].get("mapping_file_id") == "mapping-file-id-123"

    @pytest.mark.asyncio
    async def test_job_status_no_mapping_file_logs_warning(self, adapter, caplog):
        """Test that missing mapping file logs warning without failing"""
        mock_run = Mock()
        mock_run.state = "SUCCEEDED"
        mock_run.finished_at = datetime.now()
        
        job_id = "test-job-no-mapping"
        adapter._jobs[job_id] = Mock(status=JobStatus.in_progress)
        # Ensure no cached mapping_file_id is present
        adapter._job_metadata[job_id] = {"kfp_run_id": "test-run-id"}
        
        adapter.kfp_client = Mock()
        adapter.kfp_client.get_run.return_value = mock_run
        
        # Mock Files API - no mapping file returned
        from llama_stack_provider_trustyai_garak.compat import OpenAIFilePurpose
        
        mock_file_list = Mock()
        mock_file_list.data = []  # No files found
        
        adapter.file_api.openai_list_files = AsyncMock(return_value=mock_file_list)
        adapter.file_api.openai_retrieve_file_content = AsyncMock()
        
        with caplog.at_level(logging.WARNING):
            with patch.object(adapter, '_map_kfp_run_state_to_job_status', return_value=JobStatus.completed):
                result = await adapter.job_status("test-benchmark", job_id)
        
        # Should complete successfully despite missing mapping file
        assert result["status"] == JobStatus.completed
        # Should warn about missing mapping file
        assert "Could not find mapping file" in caplog.text or "mapping" in caplog.text.lower()

    @pytest.mark.asyncio
    async def test_job_status_empty_mapping_file_logs_warning(self, adapter, caplog):
        """Test that empty mapping file content logs warning without failing"""
        mock_run = Mock()
        mock_run.state = "SUCCEEDED"
        mock_run.finished_at = datetime.now()
        
        job_id = "test-job-empty-mapping"
        adapter._jobs[job_id] = Mock(status=JobStatus.in_progress)
        adapter._job_metadata[job_id] = {"kfp_run_id": "test-run-id"}
        
        adapter.kfp_client = Mock()
        adapter.kfp_client.get_run.return_value = mock_run
        
        # Mock Files API - mapping file found but empty
        from llama_stack_provider_trustyai_garak.compat import OpenAIFilePurpose
        
        mock_file_list = Mock()
        mock_file_obj = Mock()
        mock_file_obj.filename = f"{job_id}_mapping.json"
        mock_file_obj.id = "mapping-file-id-empty"
        mock_file_list.data = [mock_file_obj]
        
        # Files API returns empty content
        mock_mapping_content = Mock()
        mock_mapping_content.body.decode.return_value = ""
        
        adapter.file_api.openai_list_files = AsyncMock(return_value=mock_file_list)
        adapter.file_api.openai_retrieve_file_content = AsyncMock(return_value=mock_mapping_content)
        
        with caplog.at_level(logging.WARNING):
            with patch.object(adapter, '_map_kfp_run_state_to_job_status', return_value=JobStatus.completed):
                result = await adapter.job_status("test-benchmark", job_id)
        
        # Should complete successfully despite empty content
        assert result["status"] == JobStatus.completed
        # Should log error about JSON parsing
        assert "JSON" in caplog.text or "parse" in caplog.text.lower()

    @pytest.mark.asyncio
    async def test_job_status_invalid_mapping_file_json_logs_warning(self, adapter, caplog):
        """Test that invalid JSON in mapping file is handled gracefully"""
        mock_run = Mock()
        mock_run.state = "SUCCEEDED"
        mock_run.finished_at = datetime.now()
        
        job_id = "test-job-invalid-json"
        adapter._jobs[job_id] = Mock(status=JobStatus.in_progress)
        adapter._job_metadata[job_id] = {"kfp_run_id": "test-run-id"}
        
        adapter.kfp_client = Mock()
        adapter.kfp_client.get_run.return_value = mock_run
        
        # Mock Files API - mapping file found with invalid JSON
        from llama_stack_provider_trustyai_garak.compat import OpenAIFilePurpose
        
        mock_file_list = Mock()
        mock_file_obj = Mock()
        mock_file_obj.filename = f"{job_id}_mapping.json"
        mock_file_obj.id = "mapping-file-id-invalid-json"
        mock_file_list.data = [mock_file_obj]
        
        # Files API returns invalid JSON content
        mock_mapping_content = Mock()
        mock_mapping_content.body.decode.return_value = "not-valid-json{{{["
        
        adapter.file_api.openai_list_files = AsyncMock(return_value=mock_file_list)
        adapter.file_api.openai_retrieve_file_content = AsyncMock(return_value=mock_mapping_content)
        
        with caplog.at_level(logging.ERROR):
            with patch.object(adapter, '_map_kfp_run_state_to_job_status', return_value=JobStatus.completed):
                result = await adapter.job_status("test-benchmark", job_id)
        
        # Should complete successfully despite invalid JSON
        assert result["status"] == JobStatus.completed
        # Should log error about JSON parsing failure
        assert "Failed to parse JSON" in caplog.text or "JSON" in caplog.text

    @pytest.mark.asyncio
    async def test_job_status_uses_cached_mapping_file_id(self, adapter):
        """Test that cached mapping_file_id skips file search on subsequent calls"""
        mock_run = Mock()
        mock_run.state = "SUCCEEDED"
        mock_run.finished_at = datetime.now()
        
        job_id = "test-job-cached"
        adapter._jobs[job_id] = Mock(status=JobStatus.in_progress)
        # Pre-populate with cached mapping_file_id
        adapter._job_metadata[job_id] = {
            "kfp_run_id": "test-run-id",
            "mapping_file_id": "cached-mapping-id-456"  # Already cached!
        }
        
        adapter.kfp_client = Mock()
        adapter.kfp_client.get_run.return_value = mock_run
        
        # Mock Files API - should NOT be called for listing (uses cached ID)
        mock_mapping_content = Mock()
        mock_mapping_content.body.decode.return_value = json.dumps({
            f"{job_id}_scan_result.json": "file-id-789"
        })
        
        adapter.file_api.openai_list_files = AsyncMock()  # Should NOT be called
        adapter.file_api.openai_retrieve_file_content = AsyncMock(return_value=mock_mapping_content)
        
        with patch.object(adapter, '_map_kfp_run_state_to_job_status', return_value=JobStatus.completed):
            result = await adapter.job_status("test-benchmark", job_id)
            
            assert result["status"] == JobStatus.completed
            assert adapter._job_metadata[job_id].get(f"{job_id}_scan_result.json") == "file-id-789"
            
            # Verify we used cached ID and didn't call list_files
            adapter.file_api.openai_list_files.assert_not_called()
            # But we did retrieve content using the cached ID
            adapter.file_api.openai_retrieve_file_content.assert_called_once_with("cached-mapping-id-456")

    @pytest.mark.asyncio
    async def test_job_result_completed(self, adapter, mock_file_api, mock_benchmark):
        """Test getting results of completed job"""
        job_id = "test-job-id"
        adapter._jobs[job_id] = Mock(status=JobStatus.completed)
        adapter._job_metadata[job_id] = {
            "scan_result.json": "file-id-123"
        }
        
        # Mock file content with proper EvaluateResponse structure
        mock_response = Mock()
        mock_response.body.decode.return_value = json.dumps({
            "generations": [],  # Empty list is valid
            "scores": {}  # Empty dict is valid
        })
        mock_file_api.openai_retrieve_file_content = AsyncMock(return_value=mock_response)
        
        # Make get_benchmark async
        mock_get_benchmark = AsyncMock(return_value=mock_benchmark)
        
        with patch.object(adapter, 'get_benchmark', mock_get_benchmark):
            with patch.object(adapter, 'job_status', new_callable=AsyncMock, return_value={"status": JobStatus.completed}):
                result = await adapter.job_result("test-benchmark", job_id)
                
                # Check it's an EvaluateResponse
                assert isinstance(result, EvaluateResponse)
                assert result.generations == []
                assert result.scores == {}

    @pytest.mark.asyncio
    async def test_job_cancel(self, adapter):
        """Test job cancellation"""
        job_id = "test-job-id"
        adapter._job_metadata[job_id] = {"kfp_run_id": "test-run-id"}
        
        adapter.kfp_client = Mock()
        
        with patch.object(adapter, 'job_status', new_callable=AsyncMock, return_value={"status": JobStatus.in_progress}):
            await adapter.job_cancel("test-benchmark", job_id)
            
            adapter.kfp_client.terminate_run.assert_called_once_with("test-run-id")

    @pytest.mark.asyncio
    async def test_check_shield_availability(self, adapter):
        """Test shield availability checking"""
        adapter.shields_api = Mock()
        adapter.shields_api.get_shield = AsyncMock()
        
        # Mock shield exists
        adapter.shields_api.get_shield.return_value = {"id": "shield1"}
        
        llm_io_shield_mapping = {
            "input": ["shield1"],
            "output": ["shield2"]
        }
        
        await adapter._check_shield_availability(llm_io_shield_mapping)
        
        assert adapter.shields_api.get_shield.call_count == 2

    @pytest.mark.asyncio
    async def test_check_shield_availability_missing_shield(self, adapter):
        """Test shield availability checking with missing shield"""
        adapter.shields_api = Mock()
        adapter.shields_api.get_shield = AsyncMock(return_value=None)
        
        llm_io_shield_mapping = {
            "input": ["missing-shield"],
            "output": []
        }
        
        with pytest.raises(GarakValidationError) as exc_info:
            await adapter._check_shield_availability(llm_io_shield_mapping)
        
        assert "shield 'missing-shield' is not available" in str(exc_info.value)

    @pytest.mark.asyncio
    async def test_shutdown(self, adapter):
        """Test adapter shutdown"""
        # Add some running jobs
        job1 = Mock(status=JobStatus.in_progress)
        job2 = Mock(status=JobStatus.scheduled)
        job3 = Mock(status=JobStatus.completed)
        
        adapter._jobs = {
            "job1": job1,
            "job2": job2,
            "job3": job3
        }
        
        with patch.object(adapter, 'job_cancel', new_callable=AsyncMock) as mock_cancel:
            with patch('llama_stack_provider_trustyai_garak.shield_scan.simple_shield_orchestrator.close'):
                await adapter.shutdown()
                
                # Should cancel running and scheduled jobs
                assert mock_cancel.call_count == 2
                assert len(adapter._jobs) == 0
                assert len(adapter._job_metadata) == 0

    def test_normalize_list_arg(self, adapter):
        """Test list argument normalization"""
        assert adapter._normalize_list_arg("item1,item2") == "item1,item2"
        assert adapter._normalize_list_arg(["item1", "item2"]) == "item1,item2"

    def test_map_kfp_run_state_to_job_status(self, adapter):
        """Test mapping KFP run states to job statuses"""
        # Create mock V2beta1RuntimeState that the method imports internally
        mock_runtime_state = MagicMock()
        mock_runtime_state.RUNTIME_STATE_UNSPECIFIED = "RUNTIME_STATE_UNSPECIFIED"
        mock_runtime_state.PENDING = "PENDING"
        mock_runtime_state.RUNNING = "RUNNING"
        mock_runtime_state.SUCCEEDED = "SUCCEEDED"
        mock_runtime_state.FAILED = "FAILED"
        mock_runtime_state.CANCELED = "CANCELED"
        mock_runtime_state.CANCELING = "CANCELING"
        mock_runtime_state.PAUSED = "PAUSED"
        mock_runtime_state.SKIPPED = "SKIPPED"
        
        # Create mock kfp_server_api.models module
        mock_models = MagicMock()
        mock_models.V2beta1RuntimeState = mock_runtime_state
        
        with patch.dict('sys.modules', {'kfp_server_api': MagicMock(), 'kfp_server_api.models': mock_models}):
            assert adapter._map_kfp_run_state_to_job_status("PENDING") == JobStatus.scheduled
            assert adapter._map_kfp_run_state_to_job_status("RUNNING") == JobStatus.in_progress  
            assert adapter._map_kfp_run_state_to_job_status("SUCCEEDED") == JobStatus.completed
            assert adapter._map_kfp_run_state_to_job_status("FAILED") == JobStatus.failed
            assert adapter._map_kfp_run_state_to_job_status("CANCELED") == JobStatus.cancelled

    @pytest.mark.asyncio
    async def test_get_openai_compatible_generator_options(self, adapter, mock_benchmark_config):
        """Test OpenAI compatible generator options"""
        # Use isinstance to check for the right type
        from llama_stack_provider_trustyai_garak.compat import TopPSamplingStrategy
        
        mock_benchmark_config.eval_candidate.sampling_params.strategy = TopPSamplingStrategy(
            temperature=0.8,
            top_p=0.95
        )
        
        options = await adapter._get_openai_compatible_generator_options(
            mock_benchmark_config,
            {}
        )
        
        assert "openai" in options
        assert "OpenAICompatible" in options["openai"]
        assert options["openai"]["OpenAICompatible"]["model"] == "test-model"
        assert options["openai"]["OpenAICompatible"]["temperature"] == 0.8
        assert options["openai"]["OpenAICompatible"]["top_p"] == 0.95
        assert options["openai"]["OpenAICompatible"]["max_tokens"] == 100

    @pytest.mark.asyncio 
    async def test_get_function_based_generator_options(self, adapter, mock_benchmark_config):
        """Test function-based generator options with shields"""
        benchmark_metadata = {
            "shield_ids": ["shield1", "shield2"]
        }
        
        adapter.shields_api = Mock()
        adapter.shields_api.get_shield = AsyncMock(return_value={"id": "shield"})
        
        options = await adapter._get_function_based_generator_options(
            mock_benchmark_config,
            benchmark_metadata
        )
        
        assert "function" in options
        assert "Single" in options["function"]
        assert options["function"]["Single"]["kwargs"]["model"] == "test-model"
        assert options["function"]["Single"]["kwargs"]["llm_io_shield_mapping"]["input"] == ["shield1", "shield2"]

    @pytest.mark.asyncio
    async def test_evaluate_rows_not_implemented(self, adapter):
        """Test evaluate_rows raises NotImplementedError"""
        with pytest.raises(NotImplementedError):
            await adapter.evaluate_rows(
                "benchmark-id",
                [{"input": "test"}],
                ["score1"],
                Mock()
            )