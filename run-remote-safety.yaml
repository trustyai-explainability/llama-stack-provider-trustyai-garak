version: "1"
image_name: trustyai-garak
apis:
  - inference
  - eval
  - files
  - benchmarks
  - safety
  - shields
providers:
  inference:
    - provider_id: vllm
      provider_type: remote::vllm
      config:
        url: ${env.VLLM_URL}
        max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
        api_token: ${env.VLLM_API_TOKEN:fake}
        tls_verify: ${env.VLLM_TLS_VERIFY:=true}
  eval:
    - provider_id: trustyai_garak
      provider_type: remote::trustyai_garak
      module: llama_stack_provider_trustyai_garak.remote
      config:
        base_url: ${env.BASE_URL:=https://f70b2fb955e7.ngrok-free.app/v1} # llama-stack service base url (should be accessible from kf pods)
        timeout: ${env.GARAK_TIMEOUT:=10800} # 3 hours max timeout for garak scan
        max_concurrent_jobs: ${env.GARAK_MAX_CONCURRENT_JOBS:=5} # 5 max concurrent garak scans
        tls_verify: ${env.GARAK_TLS_VERIFY:=true}
        kubeflow_config:
          pipelines_endpoint: ${env.KUBEFLOW_PIPELINES_ENDPOINT:=https://ds-pipeline-dspa-model-namespace.apps.rosa.y1m4j9o2e1n6b9l.r6mx.p3.openshiftapps.com}
          namespace: ${env.KUBEFLOW_NAMESPACE:=model-namespace}
          experiment_name: ${env.KUBEFLOW_EXPERIMENT_NAME:=trustyai-garak-scans}
          base_image: ${env.KUBEFLOW_BASE_IMAGE:=quay.io/rh-ee-spandraj/trustyai-garak-provider-dsp:cpu}
  files:
    - provider_id: meta-reference-files
      provider_type: inline::localfs
      config:
        storage_dir: ${env.FILES_STORAGE_DIR:=~/.llama/distributions/trustyai-garak/files}
        metadata_store:
          table_name: files_metadata
          backend: sql_default
  safety:
    - provider_id: prompt-guard
      provider_type: inline::prompt-guard
      config:
        excluded_categories: []
storage:
  backends:
    kv_default:
      type: kv_sqlite
      db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/trustyai-garak}/kvstore.db
    sql_default:
      type: sql_sqlite
      db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/trustyai-garak}/sql_store.db
  stores:
    metadata:
      namespace: registry
      backend: kv_default
    inference:
      table_name: inference_store
      backend: sql_default
      max_write_queue_size: 10000
      num_writers: 4
    conversations:
      table_name: openai_conversations
      backend: sql_default
registered_resources:
  benchmarks:
    - benchmark_id: trustyai_garak::quick
      dataset_id: garak
      scoring_functions:
        - garak_scoring
      provider_id: trustyai_garak
      provider_benchmark_id: quick
    - benchmark_id: trustyai_garak::standard
      dataset_id: garak
      scoring_functions:
        - garak_scoring
      provider_id: trustyai_garak
      provider_benchmark_id: standard
    - benchmark_id: trustyai_garak::owasp_llm_top10
      dataset_id: garak
      scoring_functions:
        - garak_scoring
      provider_id: trustyai_garak
      provider_benchmark_id: owasp_llm_top10
    - benchmark_id: trustyai_garak::avid_security
      dataset_id: garak
      scoring_functions:
        - garak_scoring
      provider_id: trustyai_garak
      provider_benchmark_id: avid_security
    - benchmark_id: trustyai_garak::avid_ethics
      dataset_id: garak
      scoring_functions:
        - garak_scoring
      provider_id: trustyai_garak
      provider_benchmark_id: avid_ethics
    - benchmark_id: trustyai_garak::avid_performance
      dataset_id: garak
      scoring_functions:
        - garak_scoring
      provider_id: trustyai_garak
      provider_benchmark_id: avid_performance
  shields:
  - shield_id: Prompt-Guard-86M
    provider_id: prompt-guard
server:
  port: 8321
# external_providers_dir: ./providers.d